{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83e75e3a-7ca3-4d9c-960e-019b4dec3f10",
   "metadata": {
    "tags": []
   },
   "source": [
    "Для домашнего задания я выбрала **роман Набокова \"Приглашение на казнь\"**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae049a34-a804-43ea-8131-1017f063a9c8",
   "metadata": {},
   "source": [
    "#### 1. Убираем из текста знаки препинания и нумерацию глав (римскими цифрами) с помощью регулярных выражений:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f089ce89-ce6f-4ee0-9893-95031137a004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "with open(\"novel.txt\", \"r\", encoding = \"utf-8\") as file:\n",
    "    text = file.read()\n",
    "    # убираем знаки препинания:\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) \n",
    "    # убираем нумерацию глав:\n",
    "    text = re.sub(r'\\n[A-Z]{1,5}\\n\\n\\n','', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb63a3d-4c00-4eee-914a-61e3969aebe1",
   "metadata": {},
   "source": [
    "#### 2. Обрабатываем текст через mystem "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "d18cddee-ad4b-4f2b-807a-caf5609512d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymystem3 import Mystem\n",
    "m = Mystem()\n",
    "\n",
    "# лемматизируем текст\n",
    "lemmas = m.lemmatize(text) \n",
    "\n",
    "# делаем из lemmas сплошной текст вместо списка\n",
    "lemmas_final = ''.join(lemmas)\n",
    "\n",
    "# сохраняем в файл \n",
    "\n",
    "with open(\"mystem_result\",\"w\", encoding = \"utf-8\") as file:\n",
    "    file.write(lemmas_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c79ce7a-34ce-4681-82a4-d139513c44c6",
   "metadata": {},
   "source": [
    "#### 3. Токенизируем текст через nltk, обрабатываем через Pymorphy и сохраняем в json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "731204d6-db25-4cad-829d-a80afb31caa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import json\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "\n",
    "morph = MorphAnalyzer()\n",
    "\n",
    "# токенизируем текст:\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# создаем jsonl файл\n",
    "with open('pymorphy_result.jsonl', 'a', encoding='utf-8') as file:\n",
    "    \n",
    "# проходя по циклу дополняем файл новым словарем для каждого токена)\n",
    "\n",
    "    for token in tokens:\n",
    "        ana = morph.parse(token)[0]\n",
    "        ana_dict = {}\n",
    "        ana_dict[\"lemma\"] = ana.normal_form\n",
    "        ana_dict[\"word\"] = ana.word\n",
    "        ana_dict[\"pos\"] = ana.tag.POS\n",
    "        json.dump(ana_dict, file, ensure_ascii = False)\n",
    "        file.write(\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf82712-1e1c-4f72-a4fd-71d5f05ae78c",
   "metadata": {},
   "source": [
    "#### 4. Вопросы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a4f3df-1fb5-4bc3-bde6-c83366c7093a",
   "metadata": {},
   "source": [
    "**1. Какую долю слов составляет каждая часть речи?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a2f9a345-ebe1-4fd1-a04c-dc816b35fa05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Доля каждой из частей речи в тексте: \n",
      "\n",
      "NOUN  -  0.258069270449521\n",
      "ADJF  -  0.11748956030459347\n",
      "ADJS  -  0.009334315892901008\n",
      "COMP  -  0.00402849422746254\n",
      "VERB  -  0.13674772783099975\n",
      "INFN  -  0.02117415868337018\n",
      "PRTF  -  0.01714566445590764\n",
      "PRTS  -  0.005281257676246622\n",
      "GRND  -  0.020953082780643575\n",
      "NUMR  -  0.0039056742815033164\n",
      "ADVB  -  0.08128224023581429\n",
      "NPRO  -  0.06644559076394006\n",
      "PRED  -  0.00449521002210759\n",
      "PREP  -  0.10002456398919185\n",
      "CONJ  -  0.09882092851879146\n",
      "PRCL  -  0.04890690248096291\n",
      "INTJ  -  0.002112503070498649\n"
     ]
    }
   ],
   "source": [
    "# словарь для счета кол-ва слов определенной\n",
    "# части речи\n",
    "pos_dict = {\n",
    "    'NOUN':0,\n",
    "    'ADJF':0,\n",
    "    'ADJS':0,\n",
    "    'COMP':0,\n",
    "    'VERB':0,\n",
    "    'INFN':0,\n",
    "    'PRTF':0,\n",
    "    'PRTS':0,\n",
    "    'GRND':0,\n",
    "    'NUMR':0,\n",
    "    'ADVB':0,\n",
    "    'NPRO':0,\n",
    "    'PRED':0,\n",
    "    'PREP':0,\n",
    "    'CONJ':0,\n",
    "    'PRCL':0,\n",
    "    'INTJ':0\n",
    "}\n",
    "\n",
    "# проходим по каждому токену \n",
    "for token in tokens:\n",
    "    ana = morph.parse(token)[0]\n",
    "# обновляем счетчик частей речи в словаре pos_dict\n",
    "    if ana.tag.POS in pos_dict.keys():\n",
    "        pos_dict[ana.tag.POS] += 1\n",
    "\n",
    "print (\"Доля каждой из частей речи в тексте: \\n\")\n",
    "for key in pos_dict.keys():\n",
    "       print(key, \" - \", pos_dict[key]/len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbc61b2-afeb-4ca5-b7a9-3e2937c0f33b",
   "metadata": {},
   "source": [
    "**2. Топ-20 (по частотности) глаголов и наречий.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "fb64e9a3-b68e-4fa0-938f-befb378cd8ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-20 глаголов:\n",
      "быть 423\n",
      "сказать 204\n",
      "мочь 121\n",
      "знать 107\n",
      "хотеть 62\n",
      "говорить 57\n",
      "казаться 43\n",
      "спросить 38\n",
      "продолжать 38\n",
      "видеть 36\n",
      "сидеть 32\n",
      "смотреть 32\n",
      "стоять 30\n",
      "идти 29\n",
      "понимать 28\n",
      "выйти 26\n",
      "стать 26\n",
      "проговорить 26\n",
      "просить 26\n",
      "начать 25\n",
      "\n",
      "\n",
      "\n",
      "Топ-20 наречий:\n",
      "ещё 160\n",
      "уже 111\n",
      "только 110\n",
      "тут 93\n",
      "опять 76\n",
      "теперь 76\n",
      "где 69\n",
      "ничего 62\n",
      "там 58\n",
      "вдруг 53\n",
      "сейчас 47\n",
      "очень 42\n",
      "потом 38\n",
      "совсем 36\n",
      "наконец 34\n",
      "тогда 32\n",
      "сразу 28\n",
      "тихо 27\n",
      "затем 27\n",
      "несколько 25\n"
     ]
    }
   ],
   "source": [
    "# функция, которая помогает вывести топ-20 наречий и глаголов\n",
    "# из уже готовых, отсортированных словарей частотности \n",
    "# различных наречий и глаголов в тексте\n",
    "\n",
    "def making_top(dictionary):\n",
    "    counter = 0\n",
    "    for key in dictionary.keys():\n",
    "        print(key, dictionary[key])\n",
    "        counter += 1\n",
    "        if counter == 20:\n",
    "              break\n",
    "\n",
    "\n",
    "verb_dict = {\n",
    "}\n",
    "adverb_dict = {\n",
    "}\n",
    "\n",
    "for token in tokens:\n",
    "    ana = morph.parse(token)[0]\n",
    "    if ana.tag.POS == \"VERB\":\n",
    "        if ana.normal_form not in verb_dict.keys():\n",
    "            verb_dict[ana.normal_form] = 1\n",
    "        else:\n",
    "            verb_dict[ana.normal_form] += 1\n",
    "            \n",
    "    elif ana.tag.POS == \"ADVB\":\n",
    "        if ana.normal_form not in adverb_dict.keys():\n",
    "            adverb_dict[ana.normal_form] = 1\n",
    "        else:\n",
    "            adverb_dict[ana.normal_form] += 1\n",
    "\n",
    "#сортировка словарей по значению в порядке убывания\n",
    "sorted_verb_dict = dict(sorted(verb_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "sorted_adverb_dict = dict(sorted(adverb_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "#вывод результатов\n",
    "print(\"Топ-20 глаголов:\")\n",
    "results = making_top(sorted_verb_dict)\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "\n",
    "print(\"Топ-20 наречий:\")\n",
    "results = making_top(sorted_adverb_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22a2c03-6b16-44f4-925a-be4b171f6626",
   "metadata": {},
   "source": [
    "#### 5. Посмотрите документацию для nltk н-грамм (nltk.bigrams, например), попробуйте составить топ-25 биграмм и триграмм для вашего текста в лемматизированном виде (только леммы, без знаков препинания). Почему получаются именно такие?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7b07c668-df38-490e-8099-27b0665bd257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-25 биграмм в тексте:\n",
      "('мсье', 'пьер')  -  201\n",
      "('родрига', 'иванович')  -  55\n",
      "('сказать', 'цинциннат')  -  46\n",
      "('не', 'мочь')  -  36\n",
      "('мочь', 'быть')  -  35\n",
      "('то', 'что')  -  35\n",
      "('что', 'я')  -  34\n",
      "('сказать', 'мсье')  -  34\n",
      "('ничто', 'не')  -  33\n",
      "('то', 'быть')  -  31\n",
      "('и', 'вот')  -  31\n",
      "('я', 'не')  -  30\n",
      "('и', 'с')  -  28\n",
      "('не', 'знать')  -  28\n",
      "('к', 'цинциннат')  -  28\n",
      "('на', 'стол')  -  27\n",
      "('у', 'я')  -  27\n",
      "('как', 'бы')  -  26\n",
      "('цинциннат', 'и')  -  25\n",
      "('так', 'что')  -  25\n",
      "('спрашивать', 'цинциннат')  -  25\n",
      "('у', 'он')  -  24\n",
      "('это', 'быть')  -  24\n",
      "('и', 'не')  -  23\n",
      "('он', 'и')  -  23\n",
      "\n",
      "\n",
      "\n",
      "Топ-25 триграмм в тексте:\n",
      "('сказать', 'мсье', 'пьер')  -  34\n",
      "('мсье', 'пьер', 'и')  -  20\n",
      "('во', 'всякий', 'случай')  -  10\n",
      "('на', 'самый', 'дело')  -  8\n",
      "('и', 'мсье', 'пьер')  -  8\n",
      "('продолжать', 'мсье', 'пьер')  -  8\n",
      "('я', 'казаться', 'что')  -  7\n",
      "('о', 'то', 'что')  -  7\n",
      "('садиться', 'за', 'стол')  -  6\n",
      "('в', 'конец', 'конец')  -  6\n",
      "('сказать', 'цинциннат', 'и')  -  6\n",
      "('с', 'тот', 'же')  -  6\n",
      "('мсье', 'пьер', 'в')  -  6\n",
      "('произносить', 'мсье', 'пьер')  -  6\n",
      "('родрига', 'иванович', 'и')  -  6\n",
      "('у', 'он', 'в')  -  5\n",
      "('я', 'еще', 'не')  -  5\n",
      "('на', 'цинциннат', 'который')  -  5\n",
      "('о', 'то', 'как')  -  5\n",
      "('взглядывать', 'на', 'цинциннат')  -  5\n",
      "('я', 'знать', 'что')  -  5\n",
      "('до', 'сей', 'пора')  -  5\n",
      "('в', 'самый', 'дело')  -  5\n",
      "('не', 'знать', 'как')  -  5\n",
      "('все', 'так', 'же')  -  5\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "#функция создает словарь со всеми н-граммами в тексте и подсчитывает их частотность\n",
    "def making_ngrams_dict(lemmas_text, n):\n",
    "    ngrams_dict = {\n",
    "    }\n",
    "    ngrams_list = ngrams(lemmas_text.split(), n)\n",
    "    for ngram in ngrams_list:\n",
    "        if ngram not in ngrams_dict.keys():\n",
    "            ngrams_dict[ngram] = 1\n",
    "        else:\n",
    "            ngrams_dict[ngram] += 1\n",
    "    return ngrams_dict\n",
    "\n",
    "#функция сортирует словарь по значениям от большего к меньшему\n",
    "def sorting_dict(ngrams_dict):\n",
    "    sorted_ngrams_dict = dict(sorted(ngrams_dict.items(), key=lambda item: item[1], reverse=True))\n",
    "    return sorted_ngrams_dict\n",
    "\n",
    "#функция выводит топ-25 н-грамм\n",
    "def making_top(sorted_ngrams_dict):\n",
    "    counter = 0\n",
    "    for key in sorted_ngrams_dict.keys():\n",
    "        print(key, \" - \", sorted_ngrams_dict[key])\n",
    "        counter += 1\n",
    "        if counter == 25:\n",
    "              break\n",
    "\n",
    "            \n",
    "#используем лемматизированный текст\n",
    "with open(\"mystem_result.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    lemmas_text = file.read()\n",
    "    \n",
    "    #частотность биграмм\n",
    "    bigram_dict = making_ngrams_dict(lemmas_text, 2)\n",
    "    sorted_bigram_dict = sorting_dict(bigram_dict)\n",
    "    print(\"Топ-25 биграмм в тексте:\")\n",
    "    result = making_top(sorted_bigram_dict)\n",
    "    \n",
    "    print(\"\\n\\n\")\n",
    "    \n",
    "    #частотность триграмм\n",
    "    trigram_dict = making_ngrams_dict(lemmas_text, 3)\n",
    "    sorted_trigram_dict = sorting_dict(trigram_dict)\n",
    "    print(\"Топ-25 триграмм в тексте:\")\n",
    "    result = making_top(sorted_trigram_dict)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c923ca-2729-4bdb-b950-013b7f753e90",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Получаются такие биграммы и триграммы потому, что главными героем текста, вокруг которых строится всё повествование, являются Цинциннат, Родрига Иванович и мьсе Пьер. Их имена чаще всего встречаются на страницах книги в виде н-грамм или в составе н-грамм. \n",
    "\n",
    "Кроме того сюжет во многом строится на диалогах, размышлениях: это объясняет большую частотность н-грамм со словами 'знать', 'говорить', 'садиться'."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04919045-a49e-465a-bb60-328138e2943e",
   "metadata": {},
   "source": [
    "#### 6. Возьмите абзац из изначального текста (3 - 8 предложений), замените в нём время всех глаголов (например, прошедшее на настоящее или будущее), число всех существительных (единственное на множественное, множественное на единственное), сделайте согласование по числу глаголов и существительных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "af20db89-0427-49fc-a59a-2fa37306a214",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Он задумаются. Потом бросят карандаши, встанут, заходят. донесутся бои часа. Пользуясь их звонами, как платформами, поднимется на поверхности шаг; платформы уплывут, шаг останется, и вот в камеры войдёт: родионы с супами и господа библиотекари с каталогами. \n"
     ]
    }
   ],
   "source": [
    "paragraph = \"Он  задумался.  Потом  бросил  карандаш,  встал, \\\n",
    "заходил. Донесся  бой  часов. Пользуясь  их  звоном,  как  \\\n",
    "платформой, поднялись на поверхность шаги; платформа уплыла, \\\n",
    "шаги остались, и вот в камеру вошли:  Родион с супом и \\\n",
    "господин библиотекарь с каталогом.\"\n",
    "\n",
    "new_paragraph = \"\"\n",
    "words = word_tokenize(paragraph)\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    ana = morph.parse(word)[0]\n",
    "    \n",
    "    if 'NOUN' in ana.tag:\n",
    "        if ana.tag.number == \"sing\":\n",
    "            form = ana.inflect({'plur'})\n",
    "        elif ana.tag.number == \"plur\":\n",
    "            form = ana.inflect({'sing'})\n",
    "        \n",
    "        new_paragraph = new_paragraph + form[0] + \" \"\n",
    "        \n",
    "    elif 'VERB' in ana.tag:\n",
    "        if ana.tag.tense == \"past\":\n",
    "            if ana.tag.aspect == \"perf\":\n",
    "                pre_form = ana.inflect({'futr','3per'})\n",
    "            elif ana.tag.aspect == \"impf\":\n",
    "                pre_form = ana.inflect({'pres','3per'})\n",
    "        elif ana.tag.tense in [\"pres\", \"futr\"]:\n",
    "            pre_form = ana.inflect({'past'})\n",
    "        \n",
    "        if ana.tag.number == \"sing\":\n",
    "            form = pre_form.inflect({'plur'})\n",
    "        elif ana.tag.number == \"plur\":\n",
    "            form = pre_form.inflect({'sing'})\n",
    "    \n",
    "        new_paragraph = new_paragraph + form[0] + \" \"\n",
    "        \n",
    "    else:\n",
    "        new_paragraph = new_paragraph + word + \" \"\n",
    "\n",
    "new_paragraph = re.sub(r'\\s+(?=(?:[,.?!:;…]))','', new_paragraph)\n",
    "print(new_paragraph)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebc983a-05b3-426a-9ea1-38db72d549d7",
   "metadata": {},
   "source": [
    "При написании этого кода моя логика была такова: при изменении числа одновременно и существительного, и глагола, согласование по числу произойдет само собой. Это связано с особенностями отрывка текста, который я выбрала для задания. Однако, этот подход не учитывает наличие двух подлежащих (как в последнем предложении) и, как следствие, необходимость сохранения множественного числа глагола даже при изменении числа каждого из подлежащих.\n",
    "\n",
    "Кроме того, в результате изменения числа возникают конструкции, разительно отличающиеся по смыслу от конструкций в исходном тексте: бои часа - это не множественное число 'боя часов'.\n",
    "\n",
    "Как видно из получившегося текста, для создания грамматичного текста в результате изменениея граммем недостаточно программ, анализирующих морфологию. Важно учитывать синтаксис в том числе."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
